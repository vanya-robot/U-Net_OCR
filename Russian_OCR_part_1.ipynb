{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here we will create and train a **U-Net** Neural network to segment text from the background. Network architecture and some utility functions are presented in the **unet.py** file."
      ],
      "metadata": {
        "id": "JC1MYVWXpmiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "from unet import unet\n",
        "import unet_lines\n",
        "import unet_words"
      ],
      "metadata": {
        "id": "wae5CJlWi-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list=os.listdir('/content/drive/MyDrive/PageSegData/PageImg/') #  Insert your own path to training data\n",
        "image_list=[filename.split(\".\")[0]for filename in image_list]"
      ],
      "metadata": {
        "id": "lPL4CTOZqBgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining model. If training is already done - proceed to loading weights."
      ],
      "metadata": {
        "id": "6iWgclR7qzC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lines=unet()\n",
        "model_lines.summary()"
      ],
      "metadata": {
        "id": "PN8KfuD2tb_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split."
      ],
      "metadata": {
        "id": "3phlwYlMq2Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(image_list)\n",
        "file_train=image_list[0:int(0.75*len(image_list))]\n",
        "file_test=image_list[int(0.75*len(image_list)):]"
      ],
      "metadata": {
        "id": "ZdX2KUTrvi7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training."
      ],
      "metadata": {
        "id": "_BWJyh6tq6gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "mc = ModelCheckpoint('weights{epoch:08d}.h5', \n",
        "                                     save_weights_only=True, save_freq='epoch')\n",
        "model_lines.fit_generator(unet_lines.batch_generator(file_train,2,2),epochs=3,steps_per_epoch=1000,validation_data=unet_lines.batch_generator(file_test,2,2),\n",
        "                    validation_steps=400,callbacks=[mc],shuffle=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry7DsXtVwX_L",
        "outputId": "e43da3c9-4eaa-425a-e42e-64438fb7f3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "<ipython-input-6-f5af798bf730>:4: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(batch_generator(file_train,2,2),epochs=3,steps_per_epoch=1000,validation_data=batch_generator(file_test,2,2),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1000/1000 [==============================] - 1087s 1s/step - loss: 0.5557 - accuracy: 0.6532 - val_loss: 0.3360 - val_accuracy: 0.9269\n",
            "Epoch 2/3\n",
            "1000/1000 [==============================] - 668s 668ms/step - loss: 0.3441 - accuracy: 0.9246 - val_loss: 0.3088 - val_accuracy: 0.9342\n",
            "Epoch 3/3\n",
            "1000/1000 [==============================] - 667s 667ms/step - loss: 0.3197 - accuracy: 0.9282 - val_loss: 0.2873 - val_accuracy: 0.9368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef6e4c0d90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've already done this process, you can load pretrained weights into model here:"
      ],
      "metadata": {
        "id": "4cuBg7DhrAx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your path to weights file\n",
        "model_lines = unet(pretrained_weights='/content/drive/MyDrive/PageSegData/unet_lines.h5')"
      ],
      "metadata": {
        "id": "M0UUMRPI8khX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7cc7c1c-f135-4deb-ca5f-e0ec379fa904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the text by the lines."
      ],
      "metadata": {
        "id": "xcu2uNuKrcKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_img_array=[]\n",
        "\n",
        "\n",
        "def segment_into_lines(filename):\n",
        "    #Loading the image and performing thresholding on it and then resizing.\n",
        "    img=cv2.imread(f'{filename}',0)\n",
        "    ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
        "    img=cv2.resize(img,(512,512))\n",
        "    #Expanding the dimension to account for the batch dimension.\n",
        "    img= np.expand_dims(img,axis=-1)\n",
        "    #Expanding dimension along channel axis.\n",
        "    img=np.expand_dims(img,axis=0)\n",
        "    #Predict the segmentation mask.\n",
        "    pred=model_lines.predict(img)\n",
        "    #Remove the batch and channel dimension for performing the binarization.\n",
        "    pred=np.squeeze(np.squeeze(pred,axis=0),axis=-1)\n",
        "\n",
        "    \n",
        "    #Performing the binarization of the predicted mask for contour detection.\n",
        "    coordinates=[]\n",
        "    img = cv2.normalize(src=pred, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
        "    cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,img)\n",
        "    #Opening the original image to get the original dimension information.\n",
        "    ori_img=cv2.imread(f'{filename}',0)\n",
        " \n",
        "\n",
        "    (H, W) = ori_img.shape[:2]\n",
        "    (newW, newH) = (512, 512)\n",
        "    rW = W / float(newW)\n",
        "    rH = H / float(newH)\n",
        "    \n",
        "    #Contour detection and bouding box generation.\n",
        "    contours, hier = cv2.findContours(img, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
        "    for c in contours:\n",
        "        # get the bounding rect\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "        #cv2.rectangle(ori_img, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
        "        coordinates.append((int(x*rW),int(y*rH),int((x+w)*rW),int((y+h)*rH)))\n",
        "    \n",
        "   \n",
        "    #Cropping the lines from the original image using the bouding boxes generated above.\n",
        "    for i in range(len(coordinates)-1,-1,-1):\n",
        "        coors=coordinates[i]\n",
        "\n",
        "        p_img=ori_img[coors[1]:coors[3],coors[0]:coors[2]].copy()\n",
        "        line_img_array.append(p_img)\n",
        "        \n",
        "    return line_img_array"
      ],
      "metadata": {
        "id": "0upuUoPLtgo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we need to train another unet to split lines into words."
      ],
      "metadata": {
        "id": "JwM0FCCO2rFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_list=os.listdir('/content/drive/MyDrive/Dataset_words/img/')\n",
        "image_list=[filename.split(\".\")[0]for filename in image_list]"
      ],
      "metadata": {
        "id": "Qj8xVHTY1PWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-test split."
      ],
      "metadata": {
        "id": "YV0zt-eIEfav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(image_list)\n",
        "file_train=image_list[0:int(0.75*len(image_list))]\n",
        "file_test=image_list[int(0.75*len(image_list)):]"
      ],
      "metadata": {
        "id": "731SJEpm-f92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining model."
      ],
      "metadata": {
        "id": "adLrIPctEwbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_words = unet()\n",
        "model_words.summary()"
      ],
      "metadata": {
        "id": "dtySBo26Ei_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training."
      ],
      "metadata": {
        "id": "vRDqKZ1uWFAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "mc = ModelCheckpoint('weights{epoch:08d}.h5', \n",
        "                                     save_weights_only=True, save_freq='epoch')\n",
        "model_words.fit_generator(unet_words.batch_generator(file_train,2,2),epochs=5,steps_per_epoch=1000,validation_data=unet_words.batch_generator(file_test,2,2),\n",
        "                    validation_steps=400,callbacks=[mc],shuffle=1)"
      ],
      "metadata": {
        "id": "J0KXuUzYEvsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can insert weights of pre-trained model here."
      ],
      "metadata": {
        "id": "gD3ber4sWRUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your path to weights file\n",
        "model_words = unet(pretrained_weights='/content/drive/MyDrive/PageSegData/unet_words.h5')"
      ],
      "metadata": {
        "id": "UsTp-7ZMWNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test words segmentation model."
      ],
      "metadata": {
        "id": "ruI8fUrPYb5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_test='/content/565468.jpg'\n",
        "img=cv2.imread(f'{file_test}',0)\n",
        "img=unet_words.pad_img(img)\n",
        "ret,img=cv2.threshold(img,150,255,cv2.THRESH_BINARY_INV)\n",
        "img=cv2.resize(img,(512,512))\n",
        "img=np.expand_dims(img,axis=-1)\n",
        "#img = np.stack((img,)*3, axis=-1)\n",
        "img=img/255\n",
        "\n",
        "img=np.expand_dims(img,axis=0)\n",
        "pred=model_words.predict(img)\n",
        "pred=np.squeeze(np.squeeze(pred,axis=0),axis=-1)\n",
        "plt.imshow(pred,cmap='gray')\n",
        "\n",
        "plt.imsave('test_img_mask.jpg',pred)\n",
        "\n",
        "\n",
        "img = cv2.imread('/content/test_img_mask.jpg',0) \n",
        "cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU,img)\n",
        "ori_img=cv2.imread(f'{file_test}',0)\n",
        "ori_img=unet_words.pad_img(ori_img)\n",
        "(H, W) = ori_img.shape[:2]\n",
        "(newW, newH) = (512, 512)\n",
        "rW = W / float(newW)\n",
        "rH = H / float(newH)\n",
        "ori_img_copy=np.stack((ori_img,)*3, axis=-1)\n",
        "\n",
        "contours, hier = cv2.findContours(img, cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
        "for c in contours:\n",
        "    # get the bounding rect\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    # draw a white rectangle to visualize the bounding rect\n",
        "    cv2.rectangle(ori_img_copy, (int(x*rW), int(y*rH)), (int((x+w)*rW),int((y+h)*rH)), (255,0,0), 1)\n",
        "    #coordinates.append([x,y,(x+w),(y+h)])\n",
        "\n",
        "cv2.imwrite(\"output.png\",ori_img_copy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "HyUSUIm32U4R",
        "outputId": "c15bcc13-bf76-4628-f262-c9dea9f04fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXcUlEQVR4nO3ca4xc513H8e9/Znb25svamzR17KWJ1bRRFUFJrZKKikJRURoQyYuoCkI0iiJZKiCVFglSkJCoeFNeUKhABYtUOCkkDQ2Ro6hQTBIVVKlpN9R1LibErZvY2/W6ju3dtfc2lz8v5jnTZ88zuzt7md1Z9PtIoz33859z+Z3nnJlZc3dERGKFrS5ARLqPgkFEEgoGEUkoGEQkoWAQkYSCQUQSHQkGM7vTzF4zs9Nm9lAn1iEinWMb/T0GMysC/wt8BDgHfAf4DXd/dUNXJCId04kWw/uB0+7+A3dfAB4H7u7AekSkQ0odWOZ+4GzUfw74ueVmKBQKfvPNNzM0NISZrXnF1WqV6elparUaO3fupFwuJ8tz90XDZmZm+MEPfsD8/Dx79+5lZGSEUml9m8XdqVar1Go1CoUCpVKJQmH9GRy37rLu7L1kf+fm5jhz5gwzMzPs2LGDd7zjHfT29jbHx8tY7bbOrzMb1qrVaWbr2pet1tVKvV4HaLl96/U6lUqF8fFx3nrrLQYGBjh48CC9vb2rrqdarXLt2jXcnV27dq1qf7o7V69e5cyZM1QqFa6//nr27dtHT0/PqutYjXq9zsTEBOPj49n2vOju17dd9Ea+gHuBv4/6fwv46xbTHQZGgdFCoeDPPfecV6tVX6t6ve4XL170J5980h977DH/8Y9/7PV6PZmmVqs1h9frdT9x4oQfPHjQzcw/8YlP+MzMzJpryJZZqVR8amrKL1y44NPT0+t6X/Fyq9Vq8orfT61W85MnT/qtt97qgN97770+MTHRnCb/Wu36W81XqVT82rVrPjs76wsLC0lN63m/Ky2nXq/7/Py8LywsJNPVajVfWFjwN9980x944AE3M//Qhz7kExMTq66lVqv5hQsX/IknnvDHH3/cp6amVj3/iRMn/N3vfrebmX/2s5/16enpdW+jlVy5csU/9alPeaFQcMCBUW/zPO5Ei2EMGIn6D4Rh+UA6AhwBKJVKHodU/orUzpXH3ZmdneXMmTOUSqWWVzFYfCVzd+bm5pifnwcaV52NuLJn6ymXy5TL5Q1dZqvujLtz7do1rl27BkCtVlv3lXu5GtydSqXC5OQkAwMDDAwMNNe3VH1L1d5q2nq9TqFQWHL6bP+VSqVFrbx438dX+vn5eRYWFpY8NrJjLT++Wq1SqVSoVCpMTEw0W2PtMjMKhQLVanVR/1q0uw2z93v16tUl3+9yOvGM4TvALWZ2s5mVgfuAp1eaqVarNd9AqwTLtBqWDZ+YmOCb3/wmo6OjXL58udnMzIvnXVhYoFKprOqgXUpWV71eZ35+nvn5+TXtlHyNS22LpabPDsC5uTnq9fqy86y2vnxdc3NzjI+PMzExQa1WW3K+er1OtVpt1rPc+8m2YX76vGq1ysWLF7l48SLVarXldPV6nYWFBaBx25jt62wdtVqt+bdarTbXGa87W8bCwgKTk5PMzs4uGler1ZrzZ8dxfruXy+VFdRWLxfY2eAvLnQPxsFqtxuzs7JrWseEtBnevmtnvAl8HisCX3P2VleaLr9ZZatfr9UVXDTNrOSysl5mZGcbHx5mZmeHNN99kZGRk0f11Nl22fDOjVCo1r6z5q1Pcgmn1rCIOE3dvHmTZfe3s7Czvete7ms8YlrqK5pcT1xm/53hYfMXJllsoFOjt7W2OywIiv778evPbMp4u35+vcXp6mpMnT3Lw4EFuvPFGenp6Fm3fbLpKpcLs7GyzFZVv/cT98YlYKpUol8sUi8VFrZFse4+NjVGtVtm7d2/zZItr7e/vp7+/H2gEZXzVzp9gcc3xNikWi/T19XHdddcxOTnZPPlbba94/ng9Wf1As9Wy3PHVar9l50S8nnh7xDWst6XYiVsJ3P1rwNfWMX/zb/72Ik7j7EDIxtVqNa5cuUK1WmVhYWFRKyT/kC77WywWm83QrCkcq9frS6Z7/uDKQqFerzM9Pc3MzExSe6uDaaXkz9aV39mtdnxfX1/zoVahUFh0QK4kDs3sVSgUlr0tc288WKtUKksuDxr75urVqwwODi56GJutI54nDsJqtbrkw9tisUi1WmVqampRjfE+LpfL9PX1tTxR4pMq217xdo6PkaGhId73vvdx2223MTw8nBxH8fJa7ePe3l4GBgaa22up1uxy8id+vIxsu8V1rechekeCYS2y+8D8Fb3Vff9SaVgul+nt7aVUKrFz587kpMifmAA9PT3NA2LHjh3LXjVXktWb30Hx+KXma3Urs9TBvJz4fjsOtTicWl0h8+Jx+RZALGsiZyGUzZtfVrVaZX5+ftEnAvla8rcp2ac6rY6LrJaBgQEqlQqlUqllAMc19fT0NKdb7oraKoDL5TLDw8NUq9Xmc6O4nvz2ym8HM1sU2PF0+ZqXstKxkF9eqVRa8ycfXRMM2f1ZvFPyB3O2A7JbiXiaQqHAnj17GBkZ4dy5c4uu8q1O0my58fpabcSVrpbxMrMQy06ErNmZX1+rZeX/LhUoy52ksLgFlNXUatpWJ+RyB2arbZfNVy6X6e/vX/JkW6nl16olld27ZwGRvad4umKxyPDwcLMVstS6s/vsvr6+Zush/75WOimz4Mu3wlqFVbzsbBuVSqVFtzTrbernLfXeBwcH17S8rggGd2dsbKyZ/LB4g+fvreMAiTfI7t272blzZxIi8Y7MnxCTk5PNHZW/Zcg39Zc7qeN5CoUCg4OD7Ny5s/ncIb7CtJp3uYNkpduh+P1cuXKFmZkZgOaT8FYnXhZerQ7QuFkab5O4iV+v15mbm+Ps2bNcunQJoPmQL6stbkVk3cVicdF2zdeXTV+r1ZrHw1IPUM2MXbt2MTAw0PJCkL3Pq1evArBjx47kIeBqTs4s9FfTiovfV9ZaavWQdqUWQ6uW7EoXUHdvBuFqW79dEQz1ep1XX32Vy5cvN+8n4yZ5fPDGD2Cygywb19fXx9ve9jbOnDnTPOiyUIjvaTPu3nwWkfXHoZN/0Lncxo13UrFY5MYbb2R4eJienp7kgVGm1QPEvPhq22pcfjtmn0RA48p09erV5hdyzKz59DzbPtnw+JYteyoft8qyYdlramqKU6dO8eSTT3L27Fluu+02fvSjH2FmVCoVdu3axdDQUPNEzFqE2SteT7w/s/cVB1D2ivdjpr+/f9n9kq2j1fZajbiGdpr9sexikW2Lubm5lp/CrdSKiAMynj6bP3+8ViqVRWG9Gl0RDADHjx+np6eHt7/97ZTLZXp6ehYlfL1ep6enh5mZGWZnZymVSs2mXbYhZmZmuHz5Mv39/Tz11FN84xvfYHBwkP7+/uZBl228UqnE4OAgr732WvNjxVdeeYVjx44xNTUF/OQK2NfXB/zkipF9TyJ/xY2vqLOzs1SrVXp7e+nt7W02jbMdmX0jMg6U/P10tszsI9Us5OLnBlmLJFvu2bNnm03nN954g0ceeYQ9e/YAjSb//Pz8ohOlp6eH/v7+ReEVt3LiB6rZSVypVJiZmeH8+fO8/vrrTE9P8+ijjzI8PNxsFezYsYPdu3c3b8+q1Spzc3OUy2UGBgaaw7JgzlpZpVKpOe3c3BzQOPkHBweTEHP3ZhBm+zhu7dTrdS5fvsyZM2cAGBsb48tf/nJze+RvZ+KWZX57ZB8/F4tF+vv7m+8rnj+rK96P2TOv8+fPMzY21twvjz76KIODg81PzYrFYvOTm/wygSRU4+M/ey9ZaGfPFi5dusRLL720pkDc8B9RrYWZtSwie+PxyRIn7XplOzd7op7tpOwLT3GTLV5nXNNK8qm+2YrF4qKPOlvdymx2ffHBnN+ucS2tau2G43U1sqBY6riNwyR+rrJeS7RwX3T3Q23N3w0beqlgEJEN1XYw6B+1iEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCRWDAYz+5KZXTCzl6Nhe83suJm9Hv7uCcPNzL5gZqfN7KSZ3d7J4kWkM9ppMfwDcGdu2EPAs+5+C/Bs6Af4KHBLeB0GvrgxZYrIZloxGNz9P4FLucF3A0dD91Hgnmj4I97wLWDIzPZtVLEisjnW+ozhBncfD93ngRtC937gbDTduTAsYWaHzWzUzEbXWIOIdEhpvQtwdzczX8N8R4AjAGuZX0Q6Z60thonsFiH8vRCGjwEj0XQHwjAR2UbWGgxPA/eH7vuBY9Hwj4dPJ+4AJqNbDhHZLtx92RfwGDAOVGg8M3gQGKbxacTrwH8Ae8O0BvwN8H3gJeDQSssP87leeunV8ddoO+eju2PhxNxSesYgsiledPdD7Uyobz6KSELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhixWAwsxEze97MXjWzV8zsk2H4XjM7bmavh797wnAzsy+Y2WkzO2lmt3f6TYjIxmqnxVAFft/d3wPcAfyOmb0HeAh41t1vAZ4N/QAfBW4Jr8PAFze8ahHpqBWDwd3H3f2/Q/c0cArYD9wNHA2THQXuCd13A494w7eAITPbt+GVi0jHrOoZg5ndBPws8AJwg7uPh1HngRtC937gbDTbuTBMRLaJUrsTmtkO4Eng99x9ysya49zdzcxXs2IzO0zjVkNEukxbLQYz66ERCv/o7v8SBk9ktwjh74UwfAwYiWY/EIYt4u5H3P2Qux9aa/Ei0hntfCphwMPAKXf/i2jU08D9oft+4Fg0/OPh04k7gMnolkNEtgFzX/4OwMw+CPwX8BJQD4P/iMZzhieAnwLeAD7m7pdCkPw1cCcwAzzg7qMrrGNVtyEisiYvtttCXzEYNoOCQWRTtB0M+uajiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICIJBYOIJBQMIpJQMIhIQsEgIgkFg4gkFAwiklgxGMysz8y+bWbfM7NXzOxPw/CbzewFMzttZl8xs3IY3hv6T4fxN3X2LYjIRmunxTAPfNjdfwZ4L3Cnmd0BfA74vLu/E7gMPBimfxC4HIZ/PkwnItvIisHgDVdDb094OfBh4Kth+FHgntB9d+gnjP9lM7MNq1hEOq6tZwxmVjSzE8AF4DjwfeCKu1fDJOeA/aF7P3AWIIyfBIZbLPOwmY2a2ej63oKIbLS2gsHda+7+XuAA8H7g1vWu2N2PuPshdz+03mWJyMZa1acS7n4FeB74ADBkZqUw6gAwFrrHgBGAMH438NaGVCsim6KdTyWuN7Oh0N0PfAQ4RSMg7g2T3Q8cC91Ph37C+Ofc3TeyaBHprNLKk7APOGpmRRpB8oS7P2NmrwKPm9mfAd8FHg7TPww8amangUvAfR2oW0Q6yLrhYm5mW1+EyP9/L7b7TE/ffBSRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkYSCQUQSCgYRSSgYRCShYBCRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkYSCQUQSCgYRSSgYRCShYBCRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkUTbwWBmRTP7rpk9E/pvNrMXzOy0mX3FzMpheG/oPx3G39SZ0kWkU1bTYvgkcCrq/xzweXd/J3AZeDAMfxC4HIZ/PkwnIttIW8FgZgeAXwX+PvQb8GHgq2GSo8A9ofvu0E8Y/8thehHZJtptMfwl8AdAPfQPA1fcvRr6zwH7Q/d+4CxAGD8Zpl/EzA6b2aiZja6xdhHpkBWDwcx+Dbjg7i9u5Ird/Yi7H3L3Qxu5XBFZv1Ib0/w88OtmdhfQB+wC/goYMrNSaBUcAMbC9GPACHDOzErAbuCtDa9cRDpmxRaDu3/G3Q+4+03AfcBz7v6bwPPAvWGy+4Fjofvp0E8Y/5y7+4ZWLSIdtZ7vMfwh8GkzO03jGcLDYfjDwHAY/mngofWVKCKbzbrhYm5mW1+EyP9/L7b7TE/ffBSRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkYSCQUQSCgYRSSgYRCShYBCRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkYSCQUQSCgYRSSgYRCShYBCRhIJBRBIKBhFJKBhEJKFgEJGEgkFEEgoGEUkoGEQkoWAQkYSCQUQSbQWDmf3QzF4ysxNmNhqG7TWz42b2evi7Jww3M/uCmZ02s5Nmdnsn34CIbLzVtBh+yd3f6+6HQv9DwLPufgvwbOgH+ChwS3gdBr64UcWKyOZYz63E3cDR0H0UuCca/og3fAsYMrN961iPiGyydoPBgX83sxfN7HAYdoO7j4fu88ANoXs/cDaa91wYtoiZHTaz0ezWRES6R6nN6T7o7mNm9jbguJn9TzzS3d3MfDUrdvcjwBGA1c4rIp3VVovB3cfC3wvAU8D7gYnsFiH8vRAmHwNGotkPhGEisk2sGAxmNmhmO7Nu4FeAl4GngfvDZPcDx0L308DHw6cTdwCT0S2HiGwD7dxK3AA8ZWbZ9P/k7v9mZt8BnjCzB4E3gI+F6b8G3AWcBmaABza8ahHpKHPf+tt7M5sGXtvqOtp0HXBxq4tow3apE7ZPrdulTmhd6zvc/fp2Zm734WOnvRZ9P6Krmdnodqh1u9QJ26fW7VInrL9WfSVaRBIKBhFJdEswHNnqAlZhu9S6XeqE7VPrdqkT1llrVzx8FJHu0i0tBhHpIlseDGZ2p5m9Fn6m/dDKc3S0li+Z2QUzezka1pU/LzezETN73sxeNbNXzOyT3VivmfWZ2bfN7Huhzj8Nw282sxdCPV8xs3IY3hv6T4fxN21GnVG9RTP7rpk90+V1dvZfIbj7lr2AIvB94CBQBr4HvGcL6/kF4Hbg5WjYnwMPhe6HgM+F7ruAfwUMuAN4YZNr3QfcHrp3Av8LvKfb6g3r2xG6e4AXwvqfAO4Lw/8W+ETo/m3gb0P3fcBXNnm7fhr4J+CZ0N+tdf4QuC43bMP2/aa9kSXe3AeAr0f9nwE+s8U13ZQLhteAfaF7H43vXAD8HfAbrabborqPAR/p5nqBAeC/gZ+j8eWbUv44AL4OfCB0l8J0tkn1HaDxv0U+DDwTTqSuqzOss1UwbNi+3+pbibZ+or3F1vXz8s0QmrE/S+Nq3HX1hub5CRo/tDtOo5V4xd2rLWpp1hnGTwLDm1En8JfAHwD10D/cpXVCB/4VQqxbvvm4Lbiv/uflnWZmO4Angd9z96nwmxage+p19xrwXjMbovHr3Fu3uKSEmf0acMHdXzSzX9zqetqw4f8KIbbVLYbt8BPtrv15uZn10AiFf3T3fwmDu7Zed78CPE+jST5kZtmFKa6lWWcYvxt4axPK+3ng183sh8DjNG4n/qoL6wQ6/68QtjoYvgPcEp78lmk8xHl6i2vK68qfl1ujafAwcMrd/6Jb6zWz60NLATPrp/Ec5BSNgLh3iTqz+u8FnvNwY9xJ7v4Zdz/g7jfROA6fc/ff7LY6YZP+FcJmPSxZ5iHKXTSeqH8f+OMtruUxYByo0LgPe5DGfeOzwOvAfwB7w7QG/E2o+yXg0CbX+kEa95kngRPhdVe31Qv8NPDdUOfLwJ+E4QeBb9P4ef4/A71heF/oPx3GH9yC4+AX+cmnEl1XZ6jpe+H1SnbebOS+1zcfRSSx1bcSItKFFAwiklAwiEhCwSAiCQWDiCQUDCKSUDCISELBICKJ/wNfewk08FeSSAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a CRNN network to recognize Russian language."
      ],
      "metadata": {
        "id": "F_b1zdpIuNwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "CrerVV2ZCw5Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_list = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ0123456789'\n",
        "#CRNN model\n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        "\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        "\n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list) + 1, activation = 'softmax')(blstm_2)\n",
        "\n",
        "# model to be used at test time\n",
        "crnn_model = Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "fZQ_JVAKCzcm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crnn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMwRSdC7J339",
        "outputId": "bf66fb8c-9c4c-4456-9c79-d1e0015130d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 32, 128, 1)]      0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 32, 128, 64)       640       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 16, 64, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 16, 64, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 8, 32, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 8, 32, 256)        295168    \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 8, 32, 256)        590080    \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 4, 32, 256)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 32, 512)        1180160   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 4, 32, 512)       2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 4, 32, 512)        2359808   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4, 32, 512)       2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 2, 32, 512)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 1, 31, 512)        1049088   \n",
            "                                                                 \n",
            " lambda_1 (Lambda)           (None, 31, 512)           0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 31, 256)          656384    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 31, 256)          394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 31, 44)            11308     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,614,828\n",
            "Trainable params: 6,612,780\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst\n",
        "\n",
        "def find_dominant_color(image):\n",
        "        #Resizing parameters\n",
        "        width, height = 150,150\n",
        "        image = image.resize((width, height),resample = 0)\n",
        "        #Get colors from image object\n",
        "        pixels = image.getcolors(width * height)\n",
        "        #Sort them by count number(first element of tuple)\n",
        "        sorted_pixels = sorted(pixels, key=lambda t: t[0])\n",
        "        #Get the most frequent color\n",
        "        dominant_color = sorted_pixels[-1][1]\n",
        "        return dominant_color\n",
        "\n",
        "def preprocess_img(img, imgSize):\n",
        "    \"put img into target img of size imgSize and normalize gray-values\"\n",
        "\n",
        "    # In case of black images with no text just use black image instead.\n",
        "    if img is None:\n",
        "        img = np.zeros([imgSize[1], imgSize[0]]) \n",
        "        print(\"Image None!\")\n",
        "\n",
        "    # create target image and copy sample image into it\n",
        "    (wt, ht) = imgSize\n",
        "    (h, w) = img.shape\n",
        "    fx = w / wt\n",
        "    fy = h / ht\n",
        "    f = max(fx, fy)\n",
        "    newSize = (max(min(wt, int(w / f)), 1),\n",
        "               max(min(ht, int(h / f)), 1))  # scale according to f (result at least 1 and at most wt or ht)\n",
        "    img = cv2.resize(img, newSize, interpolation=cv2.INTER_CUBIC) \n",
        "    most_freq_pixel=find_dominant_color(Image.fromarray(img))\n",
        "    target = np.ones([ht, wt]) * most_freq_pixel  \n",
        "    target[0:newSize[1], 0:newSize[0]] = img\n",
        "\n",
        "    img = target\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "xOlMhk97bJ9o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data for training."
      ],
      "metadata": {
        "id": "v15ouLRCRxz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd data_generator\n",
        "!python generate_data.py --n_samples 300000 --word_type uppercase"
      ],
      "metadata": {
        "id": "NXsAGCUgfEI-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing for training."
      ],
      "metadata": {
        "id": "6NpomVI86LAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_img = []\n",
        "training_txt = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "orig_txt = []\n",
        " \n",
        "#lists for validation dataset\n",
        "valid_img = []\n",
        "valid_txt = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "valid_orig_txt = []\n",
        " \n",
        "max_label_len = 0\n",
        "\n",
        "annot=open('/content/annotation.txt','r').readlines()\n",
        "imagenames=[]\n",
        "txts=[]\n",
        "\n",
        "for cnt in annot:\n",
        "    filename,txt=cnt.split(',')[0],cnt.split(',')[1].split('\\n')[0]\n",
        "    imagenames.append(filename)\n",
        "    txts.append(txt)\n",
        "    \n",
        "c = list(zip(imagenames, txts))\n",
        "\n",
        "random.shuffle(c)\n",
        "\n",
        "imagenames, txts = zip(*c)\n",
        "    \n",
        "\n",
        "    \n",
        "for i in range(len(imagenames)):\n",
        "        img = cv2.imread('/content/images/'+imagenames[i],0)   \n",
        " \n",
        "        img=preprocess_img(img,(128,32))\n",
        "        img=np.expand_dims(img,axis=-1)\n",
        "        img = img/255.\n",
        "        txt = txts[i]\n",
        "        \n",
        "        # compute maximum length of the text\n",
        "        if len(txt) > max_label_len:\n",
        "            max_label_len = len(txt)\n",
        "            \n",
        "           \n",
        "        # split the 150000 data into validation and training dataset as 10% and 90% respectively\n",
        "        if i%10 == 0:     \n",
        "            valid_orig_txt.append(txt)   \n",
        "            valid_label_length.append(len(txt))\n",
        "            valid_input_length.append(31)\n",
        "            valid_img.append(img)\n",
        "            valid_txt.append(encode_to_labels(txt))\n",
        "        else:\n",
        "            orig_txt.append(txt)   \n",
        "            train_label_length.append(len(txt))\n",
        "            train_input_length.append(31)\n",
        "            training_img.append(img)\n",
        "            training_txt.append(encode_to_labels(txt)) \n",
        "        \n",
        "        # break the loop if total data is 150000\n",
        "        if i == 150000:\n",
        "            flag = 1\n",
        "            break\n",
        "        i+=1"
      ],
      "metadata": {
        "id": "8uabphukbYss"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "metadata": {
        "id": "HM--9ZGvfX8n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    #Defining the CTC loss.\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        "#CTC layer declaration using lambda.\n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "\n",
        "#Including the CTC layer to train the model.\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "metadata": {
        "id": "RBw-Rhk1fiBW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"/content/best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "3MVW8LfskgxK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "metadata": {
        "id": "LHCb6W7tkjoT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03OBA8cvpgaJ",
        "outputId": "c436a8af-805d-4acf-a78d-8081914ceaf7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training."
      ],
      "metadata": {
        "id": "rZ1sEXt86Ffn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "epochs = 15\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length],\n",
        "          y=np.zeros(len(training_img)), \n",
        "          batch_size=batch_size, epochs = epochs, \n",
        "          validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], \n",
        "          [np.zeros(len(valid_img))]), verbose = 1, callbacks = callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgujl802klk2",
        "outputId": "51401868-5dbf-48ad-f4b5-dee9fe1fdf4e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 21.4242\n",
            "Epoch 1: val_loss improved from inf to 55.48057, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 199s 340ms/step - loss: 21.4242 - val_loss: 55.4806\n",
            "Epoch 2/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.5960\n",
            "Epoch 2: val_loss improved from 55.48057 to 0.47057, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 316ms/step - loss: 0.5960 - val_loss: 0.4706\n",
            "Epoch 3/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.2612\n",
            "Epoch 3: val_loss did not improve from 0.47057\n",
            "528/528 [==============================] - 167s 316ms/step - loss: 0.2612 - val_loss: 0.5703\n",
            "Epoch 4/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1824\n",
            "Epoch 4: val_loss did not improve from 0.47057\n",
            "528/528 [==============================] - 168s 317ms/step - loss: 0.1824 - val_loss: 0.7184\n",
            "Epoch 5/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1474\n",
            "Epoch 5: val_loss improved from 0.47057 to 0.35644, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 317ms/step - loss: 0.1474 - val_loss: 0.3564\n",
            "Epoch 6/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1349\n",
            "Epoch 6: val_loss improved from 0.35644 to 0.23591, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 171s 324ms/step - loss: 0.1349 - val_loss: 0.2359\n",
            "Epoch 7/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1177\n",
            "Epoch 7: val_loss did not improve from 0.23591\n",
            "528/528 [==============================] - 168s 318ms/step - loss: 0.1177 - val_loss: 0.4043\n",
            "Epoch 8/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1082\n",
            "Epoch 8: val_loss improved from 0.23591 to 0.16361, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 316ms/step - loss: 0.1082 - val_loss: 0.1636\n",
            "Epoch 9/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.1007\n",
            "Epoch 9: val_loss improved from 0.16361 to 0.16192, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 168s 317ms/step - loss: 0.1007 - val_loss: 0.1619\n",
            "Epoch 10/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0995\n",
            "Epoch 10: val_loss improved from 0.16192 to 0.15476, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 317ms/step - loss: 0.0995 - val_loss: 0.1548\n",
            "Epoch 11/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0930\n",
            "Epoch 11: val_loss improved from 0.15476 to 0.14586, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 168s 318ms/step - loss: 0.0930 - val_loss: 0.1459\n",
            "Epoch 12/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0803\n",
            "Epoch 12: val_loss improved from 0.14586 to 0.12362, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 316ms/step - loss: 0.0803 - val_loss: 0.1236\n",
            "Epoch 13/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0822\n",
            "Epoch 13: val_loss improved from 0.12362 to 0.11507, saving model to /content/best_model.hdf5\n",
            "528/528 [==============================] - 167s 317ms/step - loss: 0.0822 - val_loss: 0.1151\n",
            "Epoch 14/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0774\n",
            "Epoch 14: val_loss did not improve from 0.11507\n",
            "528/528 [==============================] - 167s 317ms/step - loss: 0.0774 - val_loss: 0.1802\n",
            "Epoch 15/15\n",
            "528/528 [==============================] - ETA: 0s - loss: 0.0724\n",
            "Epoch 15: val_loss did not improve from 0.11507\n",
            "528/528 [==============================] - 168s 319ms/step - loss: 0.0724 - val_loss: 0.1452\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f81fd21b310>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}