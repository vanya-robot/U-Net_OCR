{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xnb1AupC8Az3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from unet import unet\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load all trained models."
   ],
   "metadata": {
    "id": "zP8TKvfB84xl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "char_list = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ0123456789'\n",
    "#CRNN model\n",
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    "\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    "\n",
    "# bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "model_crnn = Model(inputs, outputs)"
   ],
   "metadata": {
    "id": "eXdBasKw9n4d"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Insert your path to weights file\n",
    "model_lines = unet(pretrained_weights='/content/drive/MyDrive/PageSegData/unet_lines.h5')\n",
    "\n",
    "model_words = unet(pretrained_weights='/content/drive/MyDrive/PageSegData/unet_words.h5')\n",
    "\n",
    "model_crnn.load_weights('/content/drive/MyDrive/PageSegData/best_model.hdf5')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eJgNtGz83fT",
    "outputId": "a7e8591e-2e5c-4405-d482-c3546ecd1339"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing with generated data:"
   ],
   "metadata": {
    "id": "pjLgDItnESEQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python generate_data.py --n_samples 5 --word_type uppercase"
   ],
   "metadata": {
    "id": "W3CbXH9OAYp6"
   },
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pipeline_utils import preprocess_img\n",
    "\n",
    "def test_images(path, i, j):\n",
    "  imgs = []\n",
    "  for i in range(i, j):\n",
    "    img = cv2.imread(os.path.join(path, str(i) + '.jpg'), 0)\n",
    "    img=preprocess_img(img,(128,32))\n",
    "    img=np.expand_dims(img,axis=-1)\n",
    "    img = img/255.\n",
    "    img=np.expand_dims(img,axis=0)\n",
    "    imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "def test_image(path, i):\n",
    "  img = cv2.imread(os.path.join(path, str(i) + '.jpg'), 0)\n",
    "  img=preprocess_img(img,(128,32))\n",
    "  img=np.expand_dims(img,axis=-1)\n",
    "  img = img/255.\n",
    "  img=np.expand_dims(img,axis=0)\n",
    "  return img"
   ],
   "metadata": {
    "id": "xfYW1O2lCW0h"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "i = 5  # number of testing images (from data generator)\n",
    "\n",
    "# predict outputs on validation images\n",
    "for i in range(i):\n",
    "  prediction = model_crnn.predict(test_image('/content/images/', i))\n",
    "  \n",
    "  # use CTC decoder\n",
    "  out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                          greedy=True)[0][0])\n",
    "  \n",
    "  # see the results\n",
    "  with open('annotation.txt', 'r+') as file:\n",
    "      lines = file.readlines()\n",
    "      print(\"original_text =  \", lines[i].split(',')[1])\n",
    "      print(\"predicted text = \", end = '')\n",
    "      for p in out[0]:  \n",
    "          if int(p) != -1:\n",
    "              print(char_list[int(p)], end = '') \n",
    "      print('\\n')      "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRZFA5rn-R0b",
    "outputId": "22231986-c844-4995-ced2-b79ffcc25442"
   },
   "execution_count": 88,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "original_text =   И7ИЕСЙЦ\n",
      "\n",
      "predicted text = И7ИЕСЙЦ\n",
      "\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "original_text =   ЫЙЩЭ87ЛХ26ДЮ\n",
      "\n",
      "predicted text = ЫЙЩЭ87ЛХ26ДЮ\n",
      "\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "original_text =   408УЛН3ЭВНЛЦ9ЯЗ\n",
      "\n",
      "predicted text = 408УЛН3ЭВНЛЦ9ЯЗ\n",
      "\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "original_text =   ЦЧДЛВ\n",
      "\n",
      "predicted text = ЦЧДЛВ\n",
      "\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "original_text =   61Д\n",
      "\n",
      "predicted text = 61Д\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training is over. Recognition pipeline will be presented in the pipeline.py**"
   ],
   "metadata": {
    "id": "C5PIs45USEfb"
   }
  }
 ]
}
